---
icon: pen-to-square
date: 2026-01-06
category:
  - 毕业设计
tag:
  - 机器学习
---
# 机器学习
## 基础或专业术语
### 数据集(Training Set)
用于训练模型的数据集合，一般具有输入特征（x = "input" variable feature）输出特征（y = "output" variable or "target" variable）序号（m = number of training examples）。输入输出构成一条训练数据（x，y） = single training examples。（x，y）之上可以加上^(i)，用于表示这是第几个训练数据：比如
$$(x^{(i)},y^{(i)}) = i^{th} \text{\,\,training example}$$
### 输入输出和函数
机器学习中函数（Model）一般用f表示，输入（feature）常为x，真实输出为y，我们预测的输出（prediction）为$\hat{y}$（表示输入x通过函数的预测输出值）
### 如何计算模型f
我们以线性模型举例：$f_{w,b}(x) = wx + b$  我们要求模型f，实际就是要求w和b（即模型参数或者权重）的值。w和b一般就是基于x和$\hat{y}$来求得。
### 代价函数公式（cost function也可以叫成本函数）
$\hat{y} = f_{w,b}(x^{(i)})$或者$f_{w,b} = wx^{(i)}+b$
所以我们要找到w,b满足：$\text{对于所有的}(x^{(i)},y^{(i)})\text{都有}\hat{y}接近y^{(i)}$ 
代价函数就是来计算当前预测值$\hat{y}$和真实值y之间的差异.
常用的是平方误差代价函数：$$J(w,b) = \frac{1}{2m}\sum_{i = 1}^m(y - \hat{y})^2$$
或者$$J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$
这里使用2m是为了让计算更简洁，后续求权重偏导的过程中会出现一个2，在这里正好可以约去。
我们求模型的目标也就是让这个J尽可能地合理地小一些.
所以不同的权重比如w,b会有不同的J，J值与权重的映射又组成了一个函数，我们需要在这个函数中寻找J最小值所对应的权重。
#### 可视化代价函数
可以建立一个3D图，或者使用等高线来表示。
### 梯度下降（gradient decent）
所以我们要找到合适的权重就是要找到尽可能小并且合适的J，那么这就需要用一个代码可编写的算法来寻找，这就是梯度下降算法。
我的理解就是，从J函数某个位置开始作为起点，根据导数去寻找J的最小值（导数为0），要注意的是，J函数可能有多个极小值点，所以不同的起点找到的值可能不同。
算法函数：
$$w = w - \alpha \frac{\partial}{\partial w} J(w, b)$$
$w$: 权重参数 (Weight)。左侧的 $w$ 是更新后的值，右侧是当前值。
$\alpha$ (Alpha): 学习率 (Learning rate)。它决定了我们沿着梯度方向“迈步”的大小。$\frac{\partial}{\partial w} J(w, b)$: 偏导数 (Derivative)。它代表了代价函数 $J$ 在当前 $w$ 点处的斜率，指明了函数值上升最快的方向。
同理对于其他权重，比如b也有：
$$w = w - \alpha \frac{\partial}{\partial b} J(w, b)$$
要注意的是在实际的梯度下降过程中，所有的权重是同步变化的，变化顺序就要准确获取，如下：
![yqZ6tO.png](https://i.imgs.ovh/2026/01/23/yqZ6tO.png)
可见w和b是同步更新的，而不是一个更新再应用到另一个
### 学习率$\alpha$
可以理解为步长，即每次梯度下降的变化量大小。如果学习率比较大，那么值在进行梯度下降的时候一次走的值就会很大，可能会跳过一些合适的值；但是如果学习率很小，每次走的步长很小，寻找过程就会很慢。
所以我们应该寻找一个合适的学习率，或者让学习率能够采用自适应策略：因为越接近J的最小值，导数会越小，那么此时走的步长就应该小一些，此时可以让学习率根据导数的大小来变化。
利用上述公式进行计算：
$$
\begin{aligned}
\frac{\partial}{\partial w} J(w, b) &= \frac{\partial}{\partial w} \frac{1}{2m} \sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\
&= \frac{1}{2m} \sum_{i=1}^{m} \frac{\partial}{\partial w} (wx^{(i)} + b - y^{(i)})^2 \\
&= \frac{1}{2m} \sum_{i=1}^{m} 2(wx^{(i)} + b - y^{(i)}) \cdot x^{(i)} \\
&= \frac{1}{m} \sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)}
\end{aligned}
$$
$$
\begin{aligned}
\frac{\partial}{\partial b} J(w, b) &= \frac{\partial}{\partial b} \frac{1}{2m} \sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\
&= \frac{1}{2m} \sum_{i=1}^{m} 2(wx^{(i)} + b - y^{(i)}) \cdot 1 \\
&= \frac{1}{m} \sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})
\end{aligned}
$$
## 监督学习（Supervised learning 带有输入x和输出y）
- **回归算法（Regression Model）**（常用于预测数据，输出有无数种可能）
- **分类算法(Classification Model)**（顾名思义，用于分类，预测出的数据即为类别,少数可能的输出）
### 线性回归模型（Liner Regression Model）（监督学习中的回归算法）
输入只有一个维度，比方说房子的大小，输出也是一个维度，比方说房价。房价与房子大小之间的关系可以用一条线性函数来表示，通过两者关系的数据集可以拟合出一条合理的线性模型，这条线性模型就可以用来预测房价，即属于回归算法，

## 无监督学习（Unsupervised learning 只有输入x没有输出y，没有数据监督）
- 聚类算法（自己根据特征将其进行分类）
- 异常检测 （根据输入数据的特征寻找出异常的数据）
- 降维（将一个大的数据集压缩到小的数据集，但是尽可能多的保留信息）